{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rooms.dataset\n",
    "import render\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import metrics\n",
    "import train\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import trace1\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"prova\"\n",
    "\n",
    "D = rooms.dataset.dataLoader(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training parameters\n",
    "\n",
    "n_fibonacci = 128 #128 \n",
    "late_stage_model= \"UniformResidual\" #\"UniformResidual\"\n",
    "toa_perturb = True #True\n",
    "model_transmission = False #False\n",
    "\n",
    "skip_train = False #False\n",
    "continue_train = False #False\n",
    "\n",
    "n_epochs = 3 #1000 \n",
    "batch_size = 4 #4 #4 nel test\n",
    "lr = 1e-2 #1e-2\n",
    "pink_noise_supervision = True #True\n",
    "pink_start_epoch = 2 #500\n",
    "fs = 48000 #48000 \n",
    "\n",
    "load_dir= None\n",
    "save_dir= '~/prova_training_decay_loss'\n",
    "\n",
    "skip_inference = False #False\n",
    "skip_music = False #False\n",
    "skip_eval = False #False\n",
    "skip_binaural = False #False\n",
    "\n",
    "valid = False #False #Evaluate on valid instead of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = render.Renderer(n_surfaces=len(D.all_surfaces), n_fibonacci=n_fibonacci,\n",
    "                        late_stage_model=late_stage_model,\n",
    "                        toa_perturb = toa_perturb, model_transmission=model_transmission).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use multiple GPUs if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    R = nn.DataParallel(R).module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omnidirectional case\n",
    "'''\n",
    "loss_fcn = metrics.training_loss\n",
    "\n",
    "gt_audio = torch.Tensor(D.RIRs[:, :R.RIR_length])\n",
    "\n",
    "rendering_method = render.Renderer.render_RIR\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directional case\n",
    "#loss_fcn = metrics.training_loss_directional\n",
    "loss_fcn = metrics.training_loss_directional_with_decay\n",
    "\n",
    "for listener_position in D.RIRs:\n",
    "    for response in listener_position:\n",
    "        response['t_response'] = torch.Tensor(response['t_response'][:R.RIR_length])\n",
    "        response['t_response'].to(device) \n",
    "\n",
    "gt_audio = D.RIRs\n",
    "rendering_method = render.Renderer.render_RIR_directional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solo per training le prossime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(indices, source_xyz, listener_xyzs, surfaces, load_dir):\n",
    "    Ls = []\n",
    "\n",
    "    for idx in indices:\n",
    "        L= render.get_listener(source_xyz=source_xyz, listener_xyz = listener_xyzs[idx], surfaces = surfaces, \n",
    "                               load_dir = load_dir, load_num = idx, speed_of_sound = D.speed_of_sound, \n",
    "                               max_order = D.max_order, parallel_surface_pairs = D.parallel_surface_pairs, \n",
    "                               max_axial_order = D.max_axial_order)\n",
    "        Ls.append(L)\n",
    "    return Ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "if not skip_train:\n",
    "    print(\"Training\")\n",
    "\n",
    "    #Initialize Listeners\n",
    "    Ls = initialize(indices=D.train_indices,\n",
    "                    listener_xyzs=D.xyzs,\n",
    "                    source_xyz=D.speaker_xyz,\n",
    "                    surfaces=D.all_surfaces,\n",
    "                    load_dir=load_dir)\n",
    "            \n",
    "    if continue_train:\n",
    "        R.load_state_dict(torch.load(os.path.join(save_dir,\"weights.pt\"))['model_state_dict'])\n",
    "\n",
    "    losses = train.train_loop(R=R, Ls=Ls, train_gt_audio=gt_audio[D.train_indices], D=D,\n",
    "                        n_epochs = n_epochs, batch_size = batch_size, lr = lr, loss_fcn = loss_fcn,\n",
    "                        save_dir=save_dir,\n",
    "                        pink_noise_supervision = pink_noise_supervision,\n",
    "                        pink_start_epoch=pink_start_epoch,\n",
    "                        continue_train = continue_train, fs=fs)\n",
    "\n",
    "else:\n",
    "    R.load_state_dict(torch.load(os.path.join(save_dir,\"weights.pt\"))['model_state_dict'])\n",
    "    R.train = False\n",
    "    R.toa_perturb = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_file = torch.load(save_dir + '/weights.pt', map_location=device)\n",
    "R.energy_vector = nn.Parameter(pt_file['model_state_dict']['energy_vector'])\n",
    "R.source_response = nn.Parameter(pt_file['model_state_dict']['source_response'])\n",
    "R.directivity_sphere = nn.Parameter(pt_file['model_state_dict']['directivity_sphere'])\n",
    "R.decay = nn.Parameter(pt_file['model_state_dict']['decay'])\n",
    "R.RIR_residual = nn.Parameter(pt_file['model_state_dict']['RIR_residual'])\n",
    "R.spline_values = nn.Parameter(pt_file['model_state_dict']['spline_values'])\n",
    "\n",
    "R.bp_ord_cut_freqs = nn.Parameter(pt_file['model_state_dict']['bp_ord_cut_freqs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note - this function relies on precomputed reflection paths\n",
    "#########above in the original code, I added the case where there are no precomputed paths\n",
    "def inference(R, source_xyz, xyzs, load_dir, source_axis_1=None, source_axis_2=None):\n",
    "    \"\"\"\n",
    "    Render monoaural RIRs at given precomputed reflection paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    R: Renderer\n",
    "        renderer to perform inference on\n",
    "    source_xyz: np.array (3,)\n",
    "        3D location of source in meters\n",
    "    xyzs: np.array (N, 3)\n",
    "        set of listener locations to render at\n",
    "    load_dir: str\n",
    "        directory to load precomputed listener paths\n",
    "    source_axis_1: np.array (3,)\n",
    "        first axis specifying virtual source rotation,\n",
    "        default is None which is (1,0,0)\n",
    "    source_axis_2: np.array (3,)\n",
    "        second axis specifying virtual source rotation,\n",
    "        default is None which is (0,1,0)    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: np.array (N, T) of predicted RIRs    \n",
    "    \"\"\"\n",
    "\n",
    "    predictions = np.zeros((xyzs.shape[0], R.RIR_length))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        R.toa_perturb = False\n",
    "        for idx in range(xyzs.shape[0]):\n",
    "            print(idx, flush=True)\n",
    "            \n",
    "            if load_dir is None: \n",
    "                # Tracing from Scratch\n",
    "                reflections, transmissions, delays, start_directions, end_directions = (\n",
    "                    trace1.get_reflections_transmissions_and_delays(\n",
    "                    source=source_xyz, dest=xyzs[idx], surfaces=D.all_surfaces, speed_of_sound=D.speed_of_sound,\n",
    "                    max_order=D.max_order,parallel_surface_pairs=D.parallel_surface_pairs, max_axial_order=D.max_axial_order)\n",
    "                )\n",
    "                \n",
    "            else:   \n",
    "                reflections = np.load(os.path.join(load_dir, \"reflections/\"+str(idx)+\".npy\"), allow_pickle=True)\n",
    "                transmissions = np.load(os.path.join(load_dir, \"transmissions/\"+str(idx)+\".npy\"), allow_pickle=True)\n",
    "                delays = np.load(os.path.join(load_dir, \"delays/\"+str(idx)+\".npy\"),allow_pickle=True)\n",
    "                start_directions = np.load(os.path.join(load_dir, \"starts/\"+str(idx)+\".npy\"))\n",
    "\n",
    "            L = render.ListenerLocation(\n",
    "                source_xyz=source_xyz,\n",
    "                listener_xyz=xyzs[idx],\n",
    "                n_surfaces=R.n_surfaces,\n",
    "                reflections=reflections,\n",
    "                transmissions=transmissions,\n",
    "                delays=delays,\n",
    "                start_directions = start_directions)\n",
    "\n",
    "            predict = R.render_RIR(L, source_axis_1=source_axis_1, source_axis_2=source_axis_2)\n",
    "            predictions[idx] = predict.detach().cpu().numpy()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inference, rendering RIR\n",
    "\"\"\"\n",
    "R.train = False\n",
    "R.toa_perturb = False\n",
    "pred_dir = os.path.join(save_dir, \"predictions\")\n",
    "if not skip_inference:\n",
    "    pred_rirs = inference(R=R, source_xyz=D.speaker_xyz, xyzs=D.xyzs, load_dir=load_dir)\n",
    "    train.makedir_if_needed(pred_dir)\n",
    "    np.save(os.path.join(pred_dir, \"pred_rirs.npy\"), pred_rirs)\n",
    "\n",
    "    if not skip_music:\n",
    "        pred_music = evaluate.render_music(pred_rirs, D.music_dls)    \n",
    "        np.save(os.path.join(pred_dir,\"pred_musics.npy\"), pred_music)\n",
    "else:\n",
    "    pred_rirs = np.load(os.path.join(pred_dir, \"pred_rirs.npy\"))\n",
    "    pred_music = np.load(os.path.join(pred_dir, \"pred_musics.npy\"))\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Evaluation of Monoaural Audio Using Metrics\n",
    "\"\"\"\n",
    "if not skip_eval:\n",
    "    errors_dir = os.path.join(save_dir, \"errors\")\n",
    "    train.makedir_if_needed(errors_dir)\n",
    "    list_of_metrics = metrics.baseline_metrics\n",
    "\n",
    "    if valid:\n",
    "        eval_indices = D.valid_indices\n",
    "    else:\n",
    "        eval_indices = D.test_indices\n",
    "\n",
    "    # Evaluating RIR Interp\n",
    "    for eval_metric in list_of_metrics:\n",
    "\n",
    "        metric_name = eval_metric.__name__\n",
    "        errors = evaluate.compute_error(pred_rirs, gt_audio, metric=eval_metric) #original code\n",
    "        #errors = evaluate.compute_error(pred_rirs[eval_indices], gt_audio[eval_indices], metric=eval_metric)\n",
    "        \n",
    "        np.save(os.path.join(errors_dir, \"errors_\" + metric_name +\".npy\"), errors)\n",
    "        print(metric_name + \" Metric:\", flush=True)\n",
    "        print(np.mean(errors[eval_indices]))\n",
    "        #print(np.mean(errors))\n",
    "        \n",
    "    # Evaluating Music Interp\n",
    "    if not skip_music:\n",
    "        for eval_metric in list_of_metrics:\n",
    "\n",
    "            metric_name = eval_metric.__name__\n",
    "\n",
    "            # Computing Error\n",
    "            errors_music = evaluate.eval_music(pred_music, D.music, eval_metric)\n",
    "            #errors_music = evaluate.eval_music(pred_music[eval_indices], D.music[eval_indices], eval_metric)\n",
    "            \n",
    "            np.save(os.path.join(errors_dir, \"errors_music_\" + metric_name +\".npy\"), errors_music)\n",
    "            print(metric_name + \" Music Metric:\", flush=True)\n",
    "            print(np.mean(errors_music[eval_indices]))\n",
    "            #print(np.mean(errors_music))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binaural Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binauralize\n",
    "\n",
    "\"\"\"\n",
    "Binaural Rendering\n",
    "\"\"\"\n",
    "if not skip_binaural:\n",
    "    pred_binaural_RIRs = []\n",
    "    for i in range(D.bin_xyzs.shape[0]):\n",
    "        binaural_RIR_xyz = D.bin_xyzs[i]\n",
    "        bin_rir = binauralize.render_binaural(R=R, source_xyz = D.speaker_xyz,\n",
    "                                            source_axis_1=None, source_axis_2=None,\n",
    "                                            listener_xyz=binaural_RIR_xyz,\n",
    "                                            listener_forward=D.default_binaural_listener_forward, \n",
    "                                            listener_left=D.default_binaural_listener_left,\n",
    "                                            surfaces=D.all_surfaces,\n",
    "                                            speed_of_sound=D.speed_of_sound,\n",
    "                                            parallel_surface_pairs=D.parallel_surface_pairs,\n",
    "                                            max_order=D.max_order, max_axial_order=D.max_axial_order)\n",
    "\n",
    "        pred_binaural_RIRs.append(bin_rir)\n",
    "\n",
    "    pred_binaural_RIRs = np.array(pred_binaural_RIRs)\n",
    "    np.save(os.path.join(pred_dir, \"pred_bin_RIRs.npy\"), pred_binaural_RIRs)\n",
    "\n",
    "    if not skip_music:\n",
    "        pred_L = pred_binaural_RIRs[:,0,:]\n",
    "        pred_R = pred_binaural_RIRs[:,1,:]\n",
    "\n",
    "        pred_L_music = evaluate.render_music(pred_L, D.music_dls[:pred_L.shape[0]])\n",
    "        pred_R_music = evaluate.render_music(pred_R, D.music_dls[:pred_R.shape[0]])\n",
    "\n",
    "        pred_bin_music = np.stack((pred_L_music, pred_R_music), axis=2)\n",
    "        print(pred_bin_music.shape)\n",
    "        np.save(os.path.join(pred_dir, \"pred_bin_musics.npy\"), pred_bin_music)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prova di ascolto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener_1 = render.get_listener(source_xyz= np.array([5,3,1.5]), listener_xyz = np.array([5,7,1.5]), surfaces = D.all_surfaces, \n",
    "                               load_dir = load_dir, load_num = None, speed_of_sound = D.speed_of_sound, \n",
    "                               max_order = D.max_order, parallel_surface_pairs = D.parallel_surface_pairs, \n",
    "                               max_axial_order = D.max_axial_order)\n",
    "\n",
    "listener_2 = render.get_listener(source_xyz= np.array([9.9,9.9,2.9]), listener_xyz = np.array([5,3,1.5]), surfaces = D.all_surfaces, \n",
    "                               load_dir = load_dir, load_num = None, speed_of_sound = D.speed_of_sound, \n",
    "                               max_order = D.max_order, parallel_surface_pairs = D.parallel_surface_pairs, \n",
    "                               max_axial_order = D.max_axial_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIR_1 = R.render_RIR(listener_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIR_2 = R.render_RIR(listener_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIR_1 plot\n",
    "plt.plot(RIR_1.detach().cpu())\n",
    "plt.title(\"RIR_1\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIR_2\" plot\n",
    "plt.plot(RIR_2.detach().cpu())\n",
    "plt.title(\"Plot\")\n",
    "plt.xlabel(\"Indice\")\n",
    "plt.ylabel(\"Valore\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_music_1 = evaluate.render_music(np.array([RIR_1.detach().cpu()]), np.array([D.music_dls[0]]), device = device)\n",
    "predicted_music_2 = evaluate.render_music(np.array([RIR_2.detach().cpu()]), np.array([D.music_dls[0]]), device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "duration = predicted_music_1.shape[2]/fs  # Duration in seconds\n",
    "t = np.linspace(0, duration, int(fs * duration), endpoint=False)\n",
    "\n",
    "\n",
    "sd.play(predicted_music_1[0][0], samplerate=48000)\n",
    "sd.wait()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = predicted_music_2.shape[2]/fs \n",
    "t = np.linspace(0, duration, int(fs * duration), endpoint=False)\n",
    "\n",
    "\n",
    "sd.play(predicted_music_2[0][0], samplerate=48000)\n",
    "sd.wait() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(D.music_dls[9][0], samplerate=48000)\n",
    "sd.wait() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
